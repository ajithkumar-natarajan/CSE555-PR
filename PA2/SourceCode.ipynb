{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rq73_aog19vM"
   },
   "source": [
    "# CSE 555 - Introduction to pattern recognition\n",
    "## Problem Set 1: Linear Discriminant Functions and Support Vector Machines\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVhb4m-e2KVb"
   },
   "source": [
    "Dataloader to read the MNIST dataset. The dataset is read and is flattened. Hence each 28x28 image is now a vector of length 784.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0WLFzb4wasJ"
   },
   "outputs": [],
   "source": [
    "#Using data loader from previous assignment\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist(kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = ('%s-labels-idx1-ubyte.gz' % kind)\n",
    "    images_path = ('%s-images-idx3-ubyte.gz' % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "x_train, y_train = load_mnist(kind='train')\n",
    "x_test, y_test = load_mnist(kind='t10k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCWGg0l_2R77"
   },
   "source": [
    "Preprocessing the data so that the mean of the dataset is zero and the standard deviation is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bRE3ZbTAwcLz"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gY5uFWRv2ir0"
   },
   "source": [
    "The penalty is chosen as 'l1' since the requirement is to use '1-norm soft margin'.\n",
    "The regularization parameter (C) is a hyper-parameter. This is tuned using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hQVefTEY7XwS"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'C':[0.001, 0.01, 0.1, 1]}\n",
    "svc = svm.LinearSVC(loss='squared_hinge', penalty='l1', dual=False)\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "# clf = svm.LinearSVC(penalty='l1')\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSwvjHlq3Jy2"
   },
   "source": [
    "The best value for regularization parameter is obtained through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ItPxvtOu_zWa",
    "outputId": "a715fb1f-caa4-4252-d2c5-53ba82f9a080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C from cross validation:  {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best C from cross validation: \", clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_tvEUPuH3aoS"
   },
   "source": [
    "The performance of the model is evaluated through accuracy and confusion matrix as metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f6wwWt398fWO"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"score = %3.2f\" %(clf.score(x_test, y_test)))\n",
    "print(\"confusion matrix: \\n \", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given:\n",
    "<br>\n",
    "features \n",
    "\\begin{equation}\n",
    "\\left( x_{1} ,\\ y_{1}\\right) ,...,\\left( x_{N} ,\\ y_{N}\\right) \\ where\\ y_{1} ,...,y_{N} \\ \\in \\{-1,\\ 1\\}\n",
    "\\end{equation}\n",
    "<br>\n",
    "We need to minimize: \n",
    "\\begin{equation*}\n",
    "W^{T} .W+C\\sum ^{N}_{i=1} \\xi _{i}\n",
    "\\end{equation*}\n",
    "<br>\n",
    "i.e., the weighted sum between the squared length of the separating vector and the errors, where $\\displaystyle W$ is the separating vector.\n",
    "<br>\n",
    "<br>\n",
    "s.t \n",
    "\\begin{equation*}\n",
    "y_{i} .\\left( W^{T} .x_{i}\\right) \\geq 1-\\xi _{i}\n",
    "\\end{equation*}\n",
    "<br>\n",
    "and\n",
    "\\begin{gather*}\n",
    "\\xi _{i} \\geq 0\n",
    "\\end{gather*}\n",
    "<br>\n",
    "for $\\displaystyle i=1,...,N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective: To form the Lagrangian dual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using slide 4/27:\n",
    "The primal margin is :\n",
    " \n",
    "\\begin{gather*}\n",
    "\\gamma \\ =\\ \\frac{1}{\\sqrt{W^{T} .W+C\\sum ^{N}_{i=1} \\xi _{i}}}\\\\\n",
    "\\end{gather*}\n",
    "\n",
    "Using slide 5/27:\n",
    "Lagrange function:\n",
    "\n",
    "\\begin{equation*}\n",
    "L=\\overrightarrow{W}^{T}\\overrightarrow{W} +C\\sum ^{N}_{i=1} \\xi _{i} +\\sum ^{N}_{i=1} Œ±_{i}\\left( 1‚àíy_{i} \\cdot W^{T} x_{i} -\\xi _{i}\\right) -\\sum ^{N}_{i=1} \\beta _{i} \\xi _{i}\n",
    "\\end{equation*}\n",
    "with\n",
    "<br>\n",
    "Lagrange multipliers: $\\displaystyle \\alpha _{i} \\geq 0\\ ,\\ \\beta _{i} \\geq 0$,\n",
    "<br>\n",
    "Inequality constraits: $\\displaystyle 1‚àíùë¶_{i}\\left( W^{T} x_{i}\\right) ‚àí\\xi _{i} \\leq 0$ and $\\displaystyle \\xi _{i} \\geq 0$\n",
    "<br>\n",
    "for all $\\displaystyle i=1,...,N$\n",
    "#### Claim: <br>\n",
    "$\\underbrace{\\underset{\\alpha,\\beta}{max} \\underset{W,\\xi}{min}{L}}_\\text{dual solution} \\leq \\underbrace{\\underset{W,\\xi}{min} \\underset{\\alpha,\\beta}{max}{L}}_\\text{primal solution}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "L=\\overrightarrow{W}^{T}\\overrightarrow{W} +C\\sum ^{N}_{i=1} \\xi _{i} +\\sum ^{N}_{i=1} Œ±_{i}\\left( 1‚àíy_{i} \\cdot W^{T} x_{i} -\\xi _{i}\\right) -\\sum ^{N}_{i=1} \\beta _{i} \\xi _{i}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\\\\n",
    "  {\\partial_{{W}}}{L} = 2W-\\sum ^{N}_{i=1} \\alpha_i y_i \\overrightarrow{x_i} \\stackrel{\\text{set}}{=} 0 \\implies 2W = {\\sum ^{N}_{i=1} \\alpha_i y_i \\overrightarrow{x_i}}\n",
    "  \\\\\n",
    "  {\\partial_{{\\xi}}}{L} = C - \\alpha_i - \\beta_i = 0\n",
    "  \\\\ \n",
    "  \\implies 0 \\leq \\alpha_i \\leq C \\text{ and } \\beta_i \\geq 0\n",
    "\\end{equation*}\n",
    "\n",
    "Second order partial derivative \\begin{gather*} \\partial^2_w\\mathcal L=1 \\gt 0:\n",
    "\\end{gather*}\n",
    "<br>\n",
    "\\begin{gather*}\n",
    "2W=\\sum ^{N}_{i=1}\\alpha_i y_i \\vec x_i\\ \\text{minimizes L with given } \\alpha_i,\\forall i \\end{gather*}\n",
    "\n",
    "Substituting \\begin{gather*} 2W = {\\sum ^{N}_{i=1} \\alpha_i y_i \\overrightarrow{x_i}}, C=\\alpha_i + \\beta_i \\end{gather*}\n",
    "into Lagrange function, we get the dual problem of maximizing:\n",
    "\n",
    "\\begin{equation*}\n",
    "{{L}} = \\overrightarrow{W}^{T} \\overrightarrow{W} + (\\alpha_i + \\beta_i) \\sum_{i=1}^N\\xi_i + \\sum_{i=1}^N\\alpha_i(1-y_i.\\overrightarrow{W}^{T} \\overrightarrow{x_i} -\\xi_i) - \\sum_{i=1}^N \\beta_i \\xi_i\n",
    "\\\\\\quad = W^T \\overrightarrow{W} + \\alpha_i\\sum_{i=1}^N\\xi_i + \\beta_i \\sum_{i=1}^N\\xi_i +\\sum_{i=1}^N \\alpha_i - \\overrightarrow{W}^{T} \\sum_{i=1}^N \\alpha_i y_i \\overrightarrow{x_i} - \\alpha_i \\sum_{i=1}^N \\xi_i - \\beta_i \\sum_{i=1}^N \\xi_i\n",
    "\\\\\\quad = W^T \\overrightarrow{W} + \\alpha_i\\sum_{i=1}^N\\xi_i + \\beta_i \\sum_{i=1}^N\\xi_i +\\sum_{i=1}^N \\alpha_i - \\overrightarrow{W}^{T} 2\\overrightarrow{W} - \\alpha_i \\sum_{i=1}^N \\xi_i - \\beta_i \\sum_{i=1}^N \\xi_i\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\\\=\\sum_{i=1}^N \\alpha_i-\\sum_{i,j=1}^N \\overrightarrow{W}^{T}\\alpha_i y_i \\overrightarrow{x_i}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\\\=\\sum_{i=1}^N \\alpha_i-\\sum_{i,j=1}^N \\alpha_i \\alpha_j y_i y_j \\overrightarrow{x_i}^{T}\\overrightarrow{x_j}\n",
    "\\end{equation*}\n",
    "<br>\n",
    "**The dual margin is :**\n",
    " \n",
    "\\begin{gather*}\n",
    "\\gamma \\ =\\ \\frac{1}{\\sqrt{\\alpha_i \\alpha_j y_i y_j \\overrightarrow{x_i}^{T}\\overrightarrow{x_j}}}\\\\\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benefits of maximizing the margin\n",
    "1. Increasing the margin leads to ignoring the points close to the boundary. That is, the samples inside the margins are penalized less.\n",
    "2. Having a high margin in cases of data with outliers would be better so that the algorithm can generalize.\n",
    "3. Having a high margin would help to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vectors characteristics:\n",
    "1. Support vectors are the points that are crucial in building SVM.\n",
    "2. These are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane.\n",
    "3. The support vectors are those points for which the Lagrange multiplier is not zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benefits of solving the dual problem instead of the primal problem\n",
    "1. The biggest advantage of solving the dual problem comes when the kernel trick is used. It allows us to classify data that is not linearly separable in the original feature space.\n",
    "2. In dual problem, it is not necessary to explicitly compute the mapping for each data point as in the case of the primal problem.\n",
    "3. The dual space has only as many parameters as the number of data points irrespective of the dimensions.\n",
    "4. Regularizing the sparse support vector associated with the dual hypothesis is sometimes more intuitive than regularizing the vector of regression coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## PART 3\n",
    "### Objective: To formulate the primal problem and derive the dual problem if there are multiple classes. -->"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "PR_P2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
